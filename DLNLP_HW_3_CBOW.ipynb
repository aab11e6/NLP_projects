{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "080d50fc",
      "metadata": {
        "id": "080d50fc"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.datasets import DATASETS\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader\n",
        "from torch import FloatTensor as FT\n",
        "\n",
        "# Get the interactive Tools for Matplotlib\n",
        "%matplotlib notebook\n",
        "%matplotlib inline\n",
        "\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "f5419c32",
      "metadata": {
        "id": "f5419c32"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "66eb271d",
      "metadata": {
        "id": "66eb271d"
      },
      "source": [
        "### Instructions\n",
        "For this part, fill in the required code and make the notebook work. This wll be very similar to the Skip-Gram model, but a little more difficult. Look for the \"\"\" FILL IN \"\"\" string to guide you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "31b412ca",
      "metadata": {
        "id": "31b412ca"
      },
      "outputs": [],
      "source": [
        "# Where do I want to run my job. You can do \"cuda\" on linux machines\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available() else  \"cpu\"\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else  \"cpu\"\n",
        "\n",
        "# The batch size in Adam or SGD\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "# Number of epochs\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Predict from 2 words the inner word for CBOW\n",
        "# I.e. I'll have a window like [\"a\", \"b\", \"c\"] of continuous text (each is a word)\n",
        "# We'll predict each of wc = [\"a\", \"c\"] from \"b\" = wc for Skip-Gram\n",
        "# For CBOW, we'll use [\"a\", \"c\"] to predict \"b\" = wo\n",
        "WINDOW = 1\n",
        "\n",
        "# Negative samples.\n",
        "K = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "e3311ae5",
      "metadata": {
        "id": "e3311ae5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3ec9f90a",
      "metadata": {
        "id": "3ec9f90a"
      },
      "source": [
        "The text8 Wikipedia corpus. 100M characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "3217b67b",
      "metadata": {
        "id": "3217b67b",
        "outputId": "7416f651-895f-4e55-a0b4-bc5dfe7b21bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "du: cannot access 'text8': No such file or directory\n",
            "100000000\n"
          ]
        }
      ],
      "source": [
        "# Put the data in your Google Drive\n",
        "# You ca get the data here: https://www.kaggle.com/competitions/titanic/data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!du -h text8\n",
        "\n",
        "f = open('/content/drive/MyDrive/text8/text8', 'r')\n",
        "text = f.read()\n",
        "# One big string of size 100M\n",
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "6c1bf892",
      "metadata": {
        "id": "6c1bf892"
      },
      "outputs": [],
      "source": [
        "punc = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_\\'{|}~\\t\\n'\n",
        "\n",
        "# Can do regular expressions here too\n",
        "for c in punc:\n",
        "    if c in text:\n",
        "        text.replace(c, ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "451bf3ab",
      "metadata": {
        "id": "451bf3ab"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "0bc19b0c",
      "metadata": {
        "id": "0bc19b0c"
      },
      "outputs": [],
      "source": [
        "# A very crude tokenizer you get for free: lower case and also split on spaces\n",
        "TOKENIZER = get_tokenizer(\"basic_english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "ff8fd598",
      "metadata": {
        "id": "ff8fd598"
      },
      "outputs": [],
      "source": [
        "words = TOKENIZER(text)\n",
        "f = Counter(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "ba7f72f8",
      "metadata": {
        "id": "ba7f72f8",
        "outputId": "7b094847-bb20-4f3a-84ac-a086061718b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17005207"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "16b736a4",
      "metadata": {
        "id": "16b736a4"
      },
      "outputs": [],
      "source": [
        "# Do a very crude filter on the text which removes all very popular words\n",
        "text = [word for word in words if f[word] > 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "6916a763",
      "metadata": {
        "id": "6916a763",
        "outputId": "9bdd781f-9bfb-4712-8877-cb9f4bf9aa8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anarchism', 'originated', 'as', 'a', 'term']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "text[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "10ad817a",
      "metadata": {
        "id": "10ad817a"
      },
      "outputs": [],
      "source": [
        "VOCAB = build_vocab_from_iterator([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "a7f69fc9",
      "metadata": {
        "id": "a7f69fc9"
      },
      "outputs": [],
      "source": [
        "# word -> int hash map\n",
        "stoi = VOCAB.get_stoi()\n",
        "# int -> word hash map\n",
        "itos = VOCAB.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "9991f9ab",
      "metadata": {
        "id": "9991f9ab",
        "outputId": "6c22eff7-f998-448b-c12a-9815edf4f1ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "stoi['as']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "0f49f5fa",
      "metadata": {
        "id": "0f49f5fa",
        "outputId": "89f84895-e384-4439-b16f-6010bd79b0ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63641"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Total number of words\n",
        "len(stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "6c5c415a",
      "metadata": {
        "id": "6c5c415a"
      },
      "outputs": [],
      "source": [
        "f = Counter(text)\n",
        "# This is the probability that we pick a word in the corpus\n",
        "z = {word: f[word] / len(text) for word in f}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "1e543e1e",
      "metadata": {
        "id": "1e543e1e"
      },
      "outputs": [],
      "source": [
        "threshold = 1e-5\n",
        "# Probability that word is kept while subsampling\n",
        "# This is explained here and sightly differet from the paper: http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
        "p_keep = {word: (np.sqrt(z[word] / 0.001) + 1)*(0.0001 / z[word]) for word in f}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "dd22119d",
      "metadata": {
        "id": "dd22119d"
      },
      "outputs": [],
      "source": [
        "# This is in the integer space\n",
        "train_dataset = [word for word in text if random.random() < p_keep[word]]\n",
        "\n",
        "# Rebuild the vocabulary\n",
        "VOCAB = build_vocab_from_iterator([train_dataset])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "a2d695da",
      "metadata": {
        "id": "a2d695da",
        "outputId": "faa0284d-113b-41c9-e1be-5ffcb585f5b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7844729"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "30d67549",
      "metadata": {
        "id": "30d67549"
      },
      "outputs": [],
      "source": [
        "# word -> int mapping\n",
        "stoi = VOCAB.get_stoi()\n",
        "# int -> word mapping\n",
        "itos = VOCAB.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "b46337b8",
      "metadata": {
        "id": "b46337b8",
        "outputId": "b569e714-c9ef-4cfb-b9c8-85282e91cae9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63641"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# The vocabulary size after we do all the filters\n",
        "len(VOCAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "c9346f36",
      "metadata": {
        "id": "c9346f36"
      },
      "outputs": [],
      "source": [
        "# The probability we draw something for negative sampling\n",
        "f = Counter(train_dataset)\n",
        "p = torch.zeros(len(VOCAB))\n",
        "\n",
        "# Downsample frequent words and upsample less frequent\n",
        "s = sum([np.power(freq, 0.75) for word, freq in f.items()])\n",
        "\n",
        "for word in f:\n",
        "    p[stoi[word]] = np.power(f[word], 0.75) / s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "dd713c15",
      "metadata": {
        "id": "dd713c15"
      },
      "outputs": [],
      "source": [
        "# Map everything to integers\n",
        "train_dataset = [stoi[word] for word in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "8e96c0c4",
      "metadata": {
        "id": "8e96c0c4"
      },
      "outputs": [],
      "source": [
        "# This just gets the (wc, wo) pairs that are positive - they are seen together!\n",
        "def get_tokenized_dataset(dataset, verbose=False):\n",
        "    x_list = []\n",
        "\n",
        "    for i, token in enumerate(dataset):\n",
        "        m = 1\n",
        "\n",
        "        # Get the left and right tokens\n",
        "        start = max(0,i-m)\n",
        "        left_tokens = dataset[start:i]\n",
        "\n",
        "        end = min(i+m,len(dataset)-1)\n",
        "        right_tokens = dataset[i+1:end+1]\n",
        "\n",
        "        # Check these are the same length, and if so use them to add a row of data. This should be a list like\n",
        "        # [a, c, b] where b is the center word\n",
        "        if len(left_tokens) == len(right_tokens):\n",
        "            w_context = left_tokens + right_tokens\n",
        "\n",
        "            wc = token\n",
        "\n",
        "            x_list.extend(\n",
        "                [w_context + [wc]]\n",
        "            )\n",
        "\n",
        "    return x_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "3eb82aad",
      "metadata": {
        "id": "3eb82aad"
      },
      "outputs": [],
      "source": [
        "train_x_list = get_tokenized_dataset(train_dataset, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "8412ee4d",
      "metadata": {
        "id": "8412ee4d"
      },
      "outputs": [],
      "source": [
        "pickle.dump(train_x_list, open('train_x_list.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "3fd54caf",
      "metadata": {
        "id": "3fd54caf"
      },
      "outputs": [],
      "source": [
        "train_x_list = pickle.load(open('train_x_list.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "576765f3",
      "metadata": {
        "id": "576765f3",
        "outputId": "0d935491-f662-48cd-b2e4-5cfd7dceeeed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5233, 11, 3083],\n",
              " [3083, 6, 11],\n",
              " [11, 163, 6],\n",
              " [6, 1, 163],\n",
              " [163, 3133, 1],\n",
              " [1, 47, 3133],\n",
              " [3133, 56, 47],\n",
              " [47, 140, 56],\n",
              " [56, 115, 140],\n",
              " [140, 740, 115]]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "# These are (wc, wo) pairs. All are y = +1 by design\n",
        "train_x_list[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "316df1dc",
      "metadata": {
        "id": "316df1dc"
      },
      "outputs": [],
      "source": [
        "# The number of things of BATCH_SIZE = 512\n",
        "assert(len(train_x_list) // BATCH_SIZE == 32579)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "8012d03d",
      "metadata": {
        "id": "8012d03d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "488c3519",
      "metadata": {
        "id": "488c3519"
      },
      "source": [
        "### Set up the dataloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "2e31ae4b",
      "metadata": {
        "id": "2e31ae4b"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(\n",
        "    TensorDataset(\n",
        "        torch.tensor(train_x_list).to(DEVICE),\n",
        "    ),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "e9c42f50",
      "metadata": {
        "id": "e9c42f50"
      },
      "outputs": [],
      "source": [
        "for xb in train_dl:\n",
        "    assert(xb[0].shape == (BATCH_SIZE, 3))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "8c949153",
      "metadata": {
        "id": "8c949153"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "12d93d22",
      "metadata": {
        "id": "12d93d22"
      },
      "source": [
        "### Words we'll use to asses the quality of the model ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "ffada8d0",
      "metadata": {
        "id": "ffada8d0"
      },
      "outputs": [],
      "source": [
        "valid_ids = torch.tensor([\n",
        "    stoi['money'],\n",
        "    stoi['lion'],\n",
        "    stoi['africa'],\n",
        "    stoi['musician'],\n",
        "    stoi['dance'],\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "867d3a88",
      "metadata": {
        "id": "867d3a88"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "64096cd8",
      "metadata": {
        "id": "64096cd8"
      },
      "source": [
        "### Get the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "dc51c359",
      "metadata": {
        "id": "dc51c359"
      },
      "outputs": [],
      "source": [
        "class CBOWNegativeSampling(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(CBOWNegativeSampling, self).__init__()\n",
        "        self.A = nn.Embedding(vocab_size, embed_dim) # Context vectors - center word\n",
        "        self.B = nn.Embedding(vocab_size, embed_dim) # Output vectors - words around the center word\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Is this the best way? Not sure\n",
        "        initrange = 0.5\n",
        "        self.A.weight.data.uniform_(-initrange, initrange)\n",
        "        self.B.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # N is the batch size\n",
        "        # x is (N, 3)\n",
        "\n",
        "        # Context words are 2m things, m = 1 so w_context is (N, 2) while wc is (N, 1)\n",
        "        w_context, wc = x[:, :-1], x[:, -1]\n",
        "\n",
        "        # Each of these is (N, 2, D) since each context has 2 word\n",
        "        # We want this to be (N, D) and this is what we get\n",
        "\n",
        "        # (N, 2, D)\n",
        "        a = self.A(w_context)\n",
        "\n",
        "        # (N, D)\n",
        "        a_avg = a.mean(dim=1)\n",
        "\n",
        "        # Each of these is (N, D) since each target has 1 word\n",
        "        b = self.B(wc)\n",
        "\n",
        "        # The product between each context and target vector. Look at the Skip-Gram code.\n",
        "        # The logits is now (N, 1) since we sum across the final dimension.\n",
        "        logits = (a_avg * b).sum(axis=-1)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "7ae817c1",
      "metadata": {
        "id": "7ae817c1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "ed9c509d",
      "metadata": {
        "id": "ed9c509d"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate_embeddings(\n",
        "    model,\n",
        "    valid_ids,\n",
        "    itos\n",
        "):\n",
        "    \"\"\" Validation logic \"\"\"\n",
        "\n",
        "    # We will use context embeddings to get the most similar words\n",
        "    # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
        "    embedding_weights = model.A.weight\n",
        "\n",
        "    normalized_embeddings = embedding_weights.cpu() / np.sqrt(\n",
        "        np.sum(embedding_weights.cpu().numpy()**2, axis=1, keepdims=True)\n",
        "    )\n",
        "\n",
        "    # Get the embeddings corresponding to valid_term_ids\n",
        "    valid_embeddings = normalized_embeddings[valid_ids, :]\n",
        "\n",
        "    # Compute the similarity between valid_term_ids (S) and all the embeddings (V)\n",
        "    # We do S x d (d x V) => S x D and sort by negative similarity\n",
        "    top_k = 10 # Top k items will be displayed\n",
        "    similarity = np.dot(valid_embeddings.cpu().numpy(), normalized_embeddings.cpu().numpy().T)\n",
        "\n",
        "    # Invert similarity matrix to negative\n",
        "    # Ignore the first one because that would be the same word as the probe word\n",
        "    similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
        "\n",
        "    # Print the output.\n",
        "    for i, word_id in enumerate(valid_ids):\n",
        "        # j >= 1 here since we don't want to include the word itself.\n",
        "        similar_word_str = ', '.join([itos[j] for j in similarity_top_k[i, :] if j >= 1])\n",
        "        print(f\"{itos[word_id]}: {similar_word_str}\")\n",
        "\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "b0c194b8",
      "metadata": {
        "id": "b0c194b8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3b3c6ed5",
      "metadata": {
        "id": "3b3c6ed5"
      },
      "source": [
        "### Set up the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "16da95d1",
      "metadata": {
        "id": "16da95d1"
      },
      "outputs": [],
      "source": [
        "LR = 10.0\n",
        "NUM_EPOCHS = 10\n",
        "EMBED_DIM = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "cef585f4",
      "metadata": {
        "id": "cef585f4"
      },
      "outputs": [],
      "source": [
        "model = CBOWNegativeSampling(len(VOCAB), EMBED_DIM).to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "# The learning rate is lowered every epoch by 1/10\n",
        "# Is this a good idea?\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "f8a642bf",
      "metadata": {
        "id": "f8a642bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7861aa4d-e380-4f7f-a115-b0d8a8930afb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CBOWNegativeSampling(\n",
              "  (A): Embedding(63641, 300)\n",
              "  (B): Embedding(63641, 300)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "85773616",
      "metadata": {
        "id": "85773616",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce71577-8c41-41bc-dbdd-7718db8aef14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "money: worn, assures, triumvirate, anagram, walleye, unaspirated, looped, bahraini, sabines, asbestosis\n",
            "lion: unrequited, photographic, adopter, manuscript, sends, maharal, baja, dubitative, ketamine, pikes\n",
            "africa: emigrating, praised, tiberius, jaffna, idiom, marlborough, corriere, reversibility, catahoula, aris\n",
            "musician: conduct, menagerie, vl, ingenious, wineries, reffered, shugart, mackey, habilitation, modern\n",
            "dance: qxd, rms, programmability, tachycardia, iconostasis, custodian, aqueous, battletech, simo, coll\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "validate_embeddings(model, valid_ids, itos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "66075f5e",
      "metadata": {
        "id": "66075f5e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "86476e2a",
      "metadata": {
        "id": "86476e2a"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "24950481",
      "metadata": {
        "id": "24950481"
      },
      "outputs": [],
      "source": [
        "ratios = []\n",
        "\n",
        "def train(dataloader, model, optimizer, epoch):\n",
        "    model.train()\n",
        "    total_acc, total_count, total_loss, total_batches = 0, 0, 0.0, 0.0\n",
        "    log_interval = 500\n",
        "\n",
        "    for idx, x_batch in tqdm(enumerate(dataloader)):\n",
        "\n",
        "        x_batch = x_batch[0]\n",
        "\n",
        "        batch_size = x_batch.shape[0]\n",
        "\n",
        "        # Zero the gradient so they don't accumulate\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(x_batch)\n",
        "\n",
        "        # Get the positive samples loss. Notice we use weights here\n",
        "        positive_loss = torch.nn.BCEWithLogitsLoss()(input=logits, target=torch.ones(batch_size).to(DEVICE).float())\n",
        "\n",
        "        # For each batch, get some negative samples\n",
        "        # We need a total of len(y_batch) * K samples across a batch\n",
        "        # We then reshape this batch\n",
        "        # These are effectively the output words\n",
        "        negative_samples = torch.multinomial(p, batch_size * K, replacement=True)\n",
        "\n",
        "        # Context words are 2m things, m = 1 so w_context is (N, 2) while wc is (N, 1)\n",
        "        w_context, wc = x_batch[:, :-1], x_batch[:, -1]\n",
        "\n",
        "        \"\"\"\n",
        "        if w_context looks like below (batch_size = 3)\n",
        "        [\n",
        "        (a, b),\n",
        "        (c, d),\n",
        "        (e, f)\n",
        "        ] and K = 2 we'd like to get:\n",
        "\n",
        "        [\n",
        "        (a, b),\n",
        "        (a, b),\n",
        "        (c, d),\n",
        "        (c, d),\n",
        "        (e, f),\n",
        "        (e, f)\n",
        "        ]\n",
        "\n",
        "        This will be batch_size * K rows.\n",
        "        \"\"\"\n",
        "\n",
        "        # This should be (N * K, 2)\n",
        "        w_context = torch.concat([\n",
        "            w.repeat(K, 1) for w in torch.tensor(w_context).split(1)\n",
        "        ])\n",
        "\n",
        "        # Remove the last dimension 1\n",
        "        wc = negative_samples.unsqueeze(-1)\n",
        "\n",
        "        # Get the negative samples. This should be (N * K, 3)\n",
        "        # Concatenate the w_context and wc along the column. Make sure everything is on CUDA / MPS or CPU\n",
        "        x_batch_negative = torch.concat([w_context, wc.to(DEVICE)], axis=1)\n",
        "\n",
        "        \"\"\"\n",
        "        Note the way we formulated the targets: they are all 0 since these are negative samples.\n",
        "        We do the BCEWithLogitsLoss by hand basically here.\n",
        "        Notice we sum across the negative samples, per positive word.\n",
        "\n",
        "        This is literally the equation in the lecture notes.\n",
        "        \"\"\"\n",
        "\n",
        "        # (N, K, D) -> (N, D) -> (N)\n",
        "        # Look at the Skip-Gram notebook\n",
        "        negative_loss = model(x_batch_negative).neg().sigmoid().log().reshape(batch_size, K).sum(1).mean().neg().to(DEVICE)\n",
        "\n",
        "        loss = (positive_loss + negative_loss).mean()\n",
        "\n",
        "        # Get the gradients via back propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradients? Generally a good idea\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "\n",
        "        # Do an optimization step. Update the parameters A and B\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the new loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Update the batch count\n",
        "        total_batches += 1\n",
        "\n",
        "        if idx % log_interval == 0:\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
        "                \"| loss {:8.3f} \".format(\n",
        "                    epoch,\n",
        "                    idx,\n",
        "                    len(dataloader),\n",
        "                    total_loss / total_batches\n",
        "                )\n",
        "            )\n",
        "            validate_embeddings(model, valid_ids, itos)\n",
        "            total_loss, total_batches = 0.0, 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0764d675",
      "metadata": {
        "id": "0764d675"
      },
      "source": [
        "### Some results from the run look like below:\n",
        "\n",
        "Somewhere inside of 2 iterations you should get sensible associattions.\n",
        "Paste here a screenshot of the closest vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e02c09",
      "metadata": {
        "id": "a9e02c09",
        "outputId": "788a35a8-b0f7-479b-efec-3d0605344234",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]<ipython-input-73-bc078a577f1d>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  w.repeat(K, 1) for w in torch.tensor(w_context).split(1)\n",
            "1it [00:02,  2.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |     0/32580 batches | loss    4.101 \n",
            "money: worn, assures, triumvirate, anagram, walleye, unaspirated, looped, bahraini, sabines, asbestosis\n",
            "lion: unrequited, photographic, adopter, manuscript, sends, maharal, baja, dubitative, ketamine, pikes\n",
            "africa: emigrating, praised, tiberius, jaffna, idiom, marlborough, corriere, reversibility, catahoula, aris\n",
            "musician: conduct, menagerie, vl, ingenious, wineries, reffered, shugart, mackey, habilitation, modern\n",
            "dance: qxd, rms, programmability, tachycardia, iconostasis, custodian, aqueous, battletech, simo, coll\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "501it [03:15,  2.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/32580 batches | loss    3.717 \n",
            "money: worn, assures, triumvirate, walleye, anagram, looped, unaspirated, titian, bahraini, sabines\n",
            "lion: unrequited, adopter, photographic, manuscript, sends, maharal, baja, dubitative, pikes, ketamine\n",
            "africa: emigrating, praised, tiberius, jaffna, idiom, corriere, marlborough, reversibility, imprisonment, baekje\n",
            "musician: conduct, vl, menagerie, ingenious, wineries, reffered, shugart, mackey, profumo, habilitation\n",
            "dance: qxd, rms, programmability, tachycardia, iconostasis, aqueous, custodian, nia, coll, simo\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1001it [06:35,  1.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  1000/32580 batches | loss    3.352 \n",
            "money: worn, assures, triumvirate, looped, walleye, titian, anagram, unaspirated, bahraini, sabines\n",
            "lion: unrequited, photographic, adopter, manuscript, sends, baja, maharal, dubitative, pikes, ketamine\n",
            "africa: emigrating, praised, tiberius, jaffna, idiom, marlborough, baekje, corriere, reversibility, imprisonment\n",
            "musician: conduct, vl, menagerie, ingenious, wineries, reffered, mackey, profumo, shugart, metrics\n",
            "dance: qxd, rms, programmability, tachycardia, aqueous, iconostasis, custodian, nia, besiege, simo\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1501it [09:53,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  1500/32580 batches | loss    3.033 \n",
            "money: worn, looped, assures, triumvirate, walleye, anagram, titian, unaspirated, asbestosis, bahraini\n",
            "lion: unrequited, adopter, photographic, manuscript, sends, maharal, baja, dubitative, pikes, ketamine\n",
            "africa: emigrating, praised, tiberius, idiom, jaffna, baekje, marlborough, imprisonment, corriere, reversibility\n",
            "musician: conduct, ingenious, vl, menagerie, wineries, reffered, profumo, herakles, metrics, mackey\n",
            "dance: qxd, rms, programmability, tachycardia, aqueous, iconostasis, coll, custodian, presidencia, besiege\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2000it [13:20,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  2000/32580 batches | loss    2.795 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2001it [13:21,  1.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "money: worn, triumvirate, looped, assures, walleye, titian, anagram, asbestosis, bahraini, bahn\n",
            "lion: unrequited, adopter, photographic, manuscript, maharal, sends, baja, dubitative, ketamine, pikes\n",
            "africa: emigrating, praised, tiberius, idiom, jaffna, eight, marlborough, imprisonment, corriere\n",
            "musician: conduct, ingenious, vl, wineries, menagerie, reffered, profumo, controversially, habilitation, grandson\n",
            "dance: qxd, rms, programmability, tachycardia, aqueous, nia, coll, besiege, iconostasis, custodian\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2500it [16:41,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  2500/32580 batches | loss    2.623 \n",
            "money: worn, looped, assures, triumvirate, titian, anagram, walleye, bahraini, intersected, bahn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r2501it [16:42,  2.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lion: unrequited, adopter, photographic, manuscript, maharal, baja, sends, dubitative, ketamine, pikes\n",
            "africa: emigrating, be, eight, four, a, one, praised, and, or\n",
            "musician: conduct, ingenious, wineries, vl, menagerie, reffered, grandson, profumo, controversially, sergeant\n",
            "dance: qxd, rms, programmability, tachycardia, aqueous, nia, coll, besiege, custodian, increase\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3000it [20:07,  2.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  3000/32580 batches | loss    2.486 \n",
            "money: worn, looped, assures, triumvirate, titian, intersected, now, bahraini, anagram, walleye\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3001it [20:08,  2.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lion: unrequited, adopter, manuscript, photographic, maharal, sends, baja, dubitative, ketamine, pikes\n",
            "africa: more, eight, emigrating, this, and, which, one, that, a\n",
            "musician: conduct, ingenious, wineries, vl, grandson, reffered, profumo, menagerie, herakles, controversially\n",
            "dance: qxd, rms, g, programmability, tachycardia, aqueous, nia, besiege, increase, counteracting\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3500it [23:31,  2.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  3500/32580 batches | loss    2.382 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r3501it [23:31,  2.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "money: now, looped, worn, assures, triumvirate, injected, intersected, within, mur, titian\n",
            "lion: unrequited, adopter, manuscript, photographic, maharal, baja, dubitative, sends, ketamine, handkerchief\n",
            "africa: or, more, eight, that, which, four, this, nine, some\n",
            "musician: conduct, ingenious, wineries, vl, grandson, reffered, profumo, controversially, herakles, menagerie\n",
            "dance: g, qxd, rms, nia, tachycardia, besiege, programmability, aqueous, increase, counteracting\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4001it [27:00,  2.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  4000/32580 batches | loss    2.291 \n",
            "money: now, worn, within, looped, show, assures, injected, intersected, triumvirate, time\n",
            "lion: unrequited, adopter, manuscript, photographic, maharal, baja, sends, dubitative, ketamine, handkerchief\n",
            "africa: or, more, eight, zero, which, four, nine, them, and\n",
            "musician: conduct, ingenious, wineries, grandson, vl, modern, profumo, reffered, herakles, controversially\n",
            "dance: g, qxd, rms, increase, nia, besiege, tachycardia, programmability, aqueous, counteracting\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4501it [30:25,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  4500/32580 batches | loss    2.219 \n",
            "money: now, within, time, often, show, languages, when, how, looped, worn\n",
            "lion: unrequited, manuscript, adopter, photographic, maharal, baja, dubitative, sends, ketamine, handkerchief\n",
            "africa: more, eight, four, or, which, nine, zero, one, them\n",
            "musician: conduct, ingenious, grandson, modern, wineries, vl, profumo, herakles, reffered, controversially\n",
            "dance: g, qxd, increase, four, rms, power, two, people, aqueous, tachycardia\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5001it [33:48,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  5000/32580 batches | loss    2.159 \n",
            "money: now, within, time, often, when, how, show, because, languages, out\n",
            "lion: unrequited, manuscript, adopter, photographic, maharal, dubitative, baja, handkerchief, ketamine, sends\n",
            "africa: more, eight, four, or, nine, zero, and, which, south\n",
            "musician: conduct, modern, ingenious, grandson, company, wineries, vl, herakles, profumo, reffered\n",
            "dance: g, increase, power, two, four, people, qxd, no, three, rms\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5501it [37:14,  2.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  5500/32580 batches | loss    2.109 \n",
            "money: now, within, time, often, because, how, when, also, languages, show\n",
            "lion: unrequited, manuscript, adopter, photographic, maharal, handkerchief, baja, dubitative, ketamine, demian\n",
            "africa: more, eight, or, four, nine, which, south, zero, them, time\n",
            "musician: modern, conduct, company, grandson, ingenious, office, wineries, profumo, herakles, vl\n",
            "dance: g, power, increase, four, people, two, three, six, both, zero\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6000it [40:35,  2.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  6000/32580 batches | loss    2.055 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r6001it [40:35,  1.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "money: now, time, within, often, also, how, because, show, work, languages\n",
            "lion: unrequited, manuscript, adopter, maharal, photographic, handkerchief, dubitative, demian, baja, ketamine\n",
            "africa: more, eight, or, nine, time, which, south, them, zero\n",
            "musician: modern, company, conduct, grandson, office, ingenious, wineries, any, herakles, french\n",
            "dance: g, power, two, people, three, six, increase, four, both, zero\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6500it [43:58,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |  6500/32580 batches | loss    2.019 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r6501it [43:59,  1.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "money: now, time, because, within, often, how, work, made, out, also\n",
            "lion: unrequited, manuscript, adopter, maharal, photographic, handkerchief, ketamine, demian, dubitative, baja\n",
            "africa: eight, more, time, nine, south, zero, them, number, four\n",
            "musician: modern, company, conduct, grandson, any, office, french, france, ingenious, like\n",
            "dance: g, power, three, two, people, six, four, zero, both, increase\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "6924it [46:51,  2.80it/s]"
          ]
        }
      ],
      "source": [
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train(train_dl, model, optimizer, epoch)\n",
        "    # We have a learning rate scheduler here\n",
        "\n",
        "    # Basically, given the state of the optimizer, this lowers the learning rate in a smart way\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0227888",
      "metadata": {
        "id": "b0227888"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}