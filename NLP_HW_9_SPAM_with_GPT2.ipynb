{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "85c15f62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85c15f62",
        "outputId": "e5ee2232-1264-463b-f592-d4a6fb4d7212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, GPT2Config\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "import random\n",
        "from torchmetrics.classification import Recall, Accuracy, AUROC, Precision"
      ],
      "metadata": {
        "id": "X0F_Ydxvi2fe"
      },
      "id": "X0F_Ydxvi2fe",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILL = \"FILL\""
      ],
      "metadata": {
        "id": "9GPs9grPgTzU"
      },
      "id": "9GPs9grPgTzU",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3678adfa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3678adfa",
        "outputId": "4ace8990-3777-493a-896c-776e80380dc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-14 21:53:54--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘smsspamcollection.zip.14’\n",
            "\n",
            "smsspamcollection.z     [ <=>                ] 198.65K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-04-14 21:53:54 (1.35 MB/s) - ‘smsspamcollection.zip.14’ saved [203415]\n",
            "\n",
            "Archive:  smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n"
          ]
        }
      ],
      "source": [
        "!wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
        "!unzip -o smsspamcollection.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d892553d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d892553d",
        "outputId": "9e8f99dc-b627-40ba-f532-7164eaac85b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n"
          ]
        }
      ],
      "source": [
        "!unzip -o smsspamcollection.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "dd3d5e84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd3d5e84",
        "outputId": "3a50e0cc-2f72-424c-8980-8f8f52e82fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "ham\tOk lar... Joking wif u oni...\n",
            "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "ham\tU dun say so early hor... U c already then say...\n",
            "ham\tNah I don't think he goes to usf, he lives around here though\n",
            "spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
            "ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
            "ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
            "spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
            "spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
          ]
        }
      ],
      "source": [
        "!head -10 SMSSpamCollection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9b12150d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b12150d",
        "outputId": "d0569ca4-6cc7-41a1-ed9f-1dcb757b0577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-14 21:53:55--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘smsspamcollection.zip.15’\n",
            "\n",
            "smsspamcollection.z     [ <=>                ] 198.65K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-04-14 21:53:55 (1.35 MB/s) - ‘smsspamcollection.zip.15’ saved [203415]\n",
            "\n",
            "Archive:  smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n"
          ]
        }
      ],
      "source": [
        "!wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
        "!unzip -o smsspamcollection.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "98a736a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "98a736a1",
        "outputId": "17f50f3c-4300-458f-e045-915af5afa2bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                               text\n",
              "0      0  Go until jurong point, crazy.. Available only ...\n",
              "1      0                    Ok lar... Joking wif u oni...\\n\n",
              "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      0  U dun say so early hor... U c already then say...\n",
              "4      0  Nah I don't think he goes to usf, he lives aro..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-239a96cb-cbc3-455a-b575-6e2a28d0532f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-239a96cb-cbc3-455a-b575-6e2a28d0532f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-239a96cb-cbc3-455a-b575-6e2a28d0532f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-239a96cb-cbc3-455a-b575-6e2a28d0532f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a77c8f80-167a-47e0-92d1-c6a6fa9bdec0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a77c8f80-167a-47e0-92d1-c6a6fa9bdec0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a77c8f80-167a-47e0-92d1-c6a6fa9bdec0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5574,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5171,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\\n\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "file_path = 'SMSSpamCollection'\n",
        "df = pd.DataFrame({'label':int(), 'text':str()}, index = [])\n",
        "with open(file_path) as f:\n",
        "    for line in f.readlines():\n",
        "        split = line.split('\\t')\n",
        "        df = pd.concat([\n",
        "                df,\n",
        "                pd.DataFrame.from_dict({\n",
        "                    'label': [1 if split[0] == 'spam' else 0],\n",
        "                    'text': [split[1]]\n",
        "                })\n",
        "            ],\n",
        "            ignore_index=True\n",
        "        )\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "79a5cb02",
      "metadata": {
        "id": "79a5cb02"
      },
      "outputs": [],
      "source": [
        "text = df.text.values\n",
        "labels = df.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dce193a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dce193a4",
        "outputId": "46ac1322-e3d1-4a02-d3c1-d3c0aadcdc84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Set to the GPT2Tokenizer and set lower case to True\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Set the padding to 'left' or 'right'?\n",
        "# Remember we want to use the last token's embedding to represent the entire sentence\n",
        "tokenizer.padding_side = 'left'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c00a3e7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c00a3e7e",
        "outputId": "5a2fbb68-f572-44cf-86d3-5e11fddb0357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cThen i thk shd b enuff.. Still got conclusion n contents pg n references.. I'll b doing da contents pg n cover pg..\n",
            "\n",
            "╒═════════════╤═════════════╕\n",
            "│ Tokens      │   Token IDs │\n",
            "╞═════════════╪═════════════╡\n",
            "│ c           │          66 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Then        │        6423 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġi          │        1312 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġth         │         294 │\n",
            "├─────────────┼─────────────┤\n",
            "│ k           │          74 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġsh         │         427 │\n",
            "├─────────────┼─────────────┤\n",
            "│ d           │          67 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġb          │         275 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġen         │         551 │\n",
            "├─────────────┼─────────────┤\n",
            "│ uff         │        1648 │\n",
            "├─────────────┼─────────────┤\n",
            "│ ..          │         492 │\n",
            "├─────────────┼─────────────┤\n",
            "│ ĠStill      │        7831 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġgot        │        1392 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġconclusion │        7664 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġn          │         299 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġcontents   │       10154 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġpg         │       23241 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġn          │         299 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġreferences │       10288 │\n",
            "├─────────────┼─────────────┤\n",
            "│ ..          │         492 │\n",
            "├─────────────┼─────────────┤\n",
            "│ ĠI          │         314 │\n",
            "├─────────────┼─────────────┤\n",
            "│ 'll         │        1183 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġb          │         275 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġdoing      │        1804 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġda         │       12379 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġcontents   │       10154 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġpg         │       23241 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġn          │         299 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġcover      │        3002 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ġpg         │       23241 │\n",
            "├─────────────┼─────────────┤\n",
            "│ ..          │         492 │\n",
            "├─────────────┼─────────────┤\n",
            "│ Ċ           │         198 │\n",
            "╘═════════════╧═════════════╛\n"
          ]
        }
      ],
      "source": [
        "def print_rand_sentence():\n",
        "    '''Displays the tokens and respective IDs of a random text sample'''\n",
        "    index = random.randint(0, len(text)-1)\n",
        "    print(text[index])\n",
        "    table = np.array([tokenizer.tokenize(text[index]),\n",
        "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
        "    print(tabulate(table,\n",
        "                 headers = ['Tokens', 'Token IDs'],\n",
        "                 tablefmt = 'fancy_grid'))\n",
        "\n",
        "print_rand_sentence()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e85d0895",
      "metadata": {
        "id": "e85d0895"
      },
      "outputs": [],
      "source": [
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  # Use the tokenizer and the encode_plus methods to return the right data we'll need\n",
        "  # Set max_length = 32 and return_tokens = 'pt'\n",
        "  # Set other fields to the appropriate booleans needed\n",
        "  return tokenizer.encode_plus(\n",
        "        input_text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=32,\n",
        "        return_tensors='pt',\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "\n",
        "for sample in text:\n",
        "    encoding_dict = preprocessing(sample, tokenizer)\n",
        "    token_id.append(encoding_dict['input_ids'])\n",
        "    attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "\n",
        "# Gather all the torch_id, attention masks, and labels\n",
        "token_id = torch.cat(token_id, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "222e06a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "222e06a7",
        "outputId": "44e6b2a9-71ed-4849-e5ce-d523d7599459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', 'Hey', 'Ġare', 'Ġwe', 'Ġgoing', 'Ġfor', 'Ġthe', 'Ġlo', 'Ġlesson', 'Ġor', 'Ġgym', '?', 'Ġ', 'Ċ']\n",
            "╒═══════════════╤═════════════╤══════════════════╕\n",
            "│ Tokens        │   Token IDs │   Attention Mask │\n",
            "╞═══════════════╪═════════════╪══════════════════╡\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ <|endoftext|> │       50256 │                0 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ Hey           │       10814 │                1 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ Ġare          │         389 │                1 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ Ġwe           │         356 │                1 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ Ġgoing        │        1016 │                1 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ Ġfor          │         329 │                1 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ Ġthe          │         262 │                1 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ Ġlo           │        2376 │                1 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ Ġlesson       │       11483 │                1 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ Ġor           │         393 │                1 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ Ġgym          │       11550 │                1 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ ?             │          30 │                1 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ Ġ             │         220 │                1 │\n",
            "├───────────────┼─────────────┼──────────────────┤\n",
            "│ Ċ             │         198 │                1 │\n",
            "╘═══════════════╧═════════════╧══════════════════╛\n"
          ]
        }
      ],
      "source": [
        "def print_rand_sentence_encoding():\n",
        "    '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
        "    index = random.randint(0, len(text) - 1)\n",
        "    tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
        "    print(tokens)\n",
        "    token_ids = [i.numpy() for i in token_id[index]]\n",
        "    attention = [i.numpy() for i in attention_masks[index]]\n",
        "    table = np.array([tokens, token_ids, attention]).T\n",
        "    print(\n",
        "        tabulate(\n",
        "            table,\n",
        "            headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
        "            tablefmt = 'fancy_grid')\n",
        "    )\n",
        "\n",
        "print_rand_sentence_encoding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e1c2c10b",
      "metadata": {
        "id": "e1c2c10b"
      },
      "outputs": [],
      "source": [
        "val_ratio = 0.2\n",
        "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "batch_size = 16\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "# Use train_test_split\n",
        "train_idx, val_idx = train_test_split(\n",
        "    range(len(labels)),  # generate indices from the length of the labels\n",
        "    test_size=val_ratio, # size of validation set\n",
        "    stratify=labels,     # stratify by labels to ensure balanced classes\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train and validation sets\n",
        "# Set to TensorDataset\n",
        "train_set = TensorDataset(token_id[train_idx], attention_masks[train_idx], labels[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx], attention_masks[val_idx], labels[val_idx])\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "validation_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "155cdefa",
      "metadata": {
        "id": "155cdefa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "40bcf8c4",
      "metadata": {
        "id": "40bcf8c4"
      },
      "source": [
        "### Load specific versions of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f85fc88e",
      "metadata": {
        "id": "f85fc88e"
      },
      "outputs": [],
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "# Do not ouput the attentions and all hidden states\n",
        "\n",
        "config = GPT2Config.from_pretrained('gpt2',\n",
        "                                    output_attentions=False,\n",
        "                                    output_hidden_states=False,\n",
        "                                    num_labels=2)\n",
        "\n",
        "# Set to 'gpt2' (the smallest GPT2 which is 120 M parameters)\n",
        "# Use the config above and set other labels as needed\n",
        "model = GPT2ForSequenceClassification(config)\n",
        "\n",
        "# Set the pad token id to the eos token id\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5\n",
        "# See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr = 5e-5,\n",
        "    eps = 1e-08\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c5ea7837",
      "metadata": {
        "id": "c5ea7837"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a36b60bf",
      "metadata": {
        "id": "a36b60bf"
      },
      "source": [
        "### Set the model to the right device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "071b1a84",
      "metadata": {
        "id": "071b1a84"
      },
      "outputs": [],
      "source": [
        "# If on GPU, do as below\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4811b9f5",
      "metadata": {
        "id": "4811b9f5"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "\n",
        "# Recommended number of epochs: See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f9f4e75e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9f4e75e",
        "outputId": "81d41723-5060-4706-9505-779e57904001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer.wte.weight: 38597376\n",
            "transformer.wpe.weight: 786432\n",
            "transformer.h.0.ln_1.weight: 768\n",
            "transformer.h.0.ln_1.bias: 768\n",
            "transformer.h.0.attn.c_attn.weight: 1769472\n",
            "transformer.h.0.attn.c_attn.bias: 2304\n",
            "transformer.h.0.attn.c_proj.weight: 589824\n",
            "transformer.h.0.attn.c_proj.bias: 768\n",
            "transformer.h.0.ln_2.weight: 768\n",
            "transformer.h.0.ln_2.bias: 768\n",
            "transformer.h.0.mlp.c_fc.weight: 2359296\n",
            "transformer.h.0.mlp.c_fc.bias: 3072\n",
            "transformer.h.0.mlp.c_proj.weight: 2359296\n",
            "transformer.h.0.mlp.c_proj.bias: 768\n",
            "transformer.h.1.ln_1.weight: 768\n",
            "transformer.h.1.ln_1.bias: 768\n",
            "transformer.h.1.attn.c_attn.weight: 1769472\n",
            "transformer.h.1.attn.c_attn.bias: 2304\n",
            "transformer.h.1.attn.c_proj.weight: 589824\n",
            "transformer.h.1.attn.c_proj.bias: 768\n",
            "transformer.h.1.ln_2.weight: 768\n",
            "transformer.h.1.ln_2.bias: 768\n",
            "transformer.h.1.mlp.c_fc.weight: 2359296\n",
            "transformer.h.1.mlp.c_fc.bias: 3072\n",
            "transformer.h.1.mlp.c_proj.weight: 2359296\n",
            "transformer.h.1.mlp.c_proj.bias: 768\n",
            "transformer.h.2.ln_1.weight: 768\n",
            "transformer.h.2.ln_1.bias: 768\n",
            "transformer.h.2.attn.c_attn.weight: 1769472\n",
            "transformer.h.2.attn.c_attn.bias: 2304\n",
            "transformer.h.2.attn.c_proj.weight: 589824\n",
            "transformer.h.2.attn.c_proj.bias: 768\n",
            "transformer.h.2.ln_2.weight: 768\n",
            "transformer.h.2.ln_2.bias: 768\n",
            "transformer.h.2.mlp.c_fc.weight: 2359296\n",
            "transformer.h.2.mlp.c_fc.bias: 3072\n",
            "transformer.h.2.mlp.c_proj.weight: 2359296\n",
            "transformer.h.2.mlp.c_proj.bias: 768\n",
            "transformer.h.3.ln_1.weight: 768\n",
            "transformer.h.3.ln_1.bias: 768\n",
            "transformer.h.3.attn.c_attn.weight: 1769472\n",
            "transformer.h.3.attn.c_attn.bias: 2304\n",
            "transformer.h.3.attn.c_proj.weight: 589824\n",
            "transformer.h.3.attn.c_proj.bias: 768\n",
            "transformer.h.3.ln_2.weight: 768\n",
            "transformer.h.3.ln_2.bias: 768\n",
            "transformer.h.3.mlp.c_fc.weight: 2359296\n",
            "transformer.h.3.mlp.c_fc.bias: 3072\n",
            "transformer.h.3.mlp.c_proj.weight: 2359296\n",
            "transformer.h.3.mlp.c_proj.bias: 768\n",
            "transformer.h.4.ln_1.weight: 768\n",
            "transformer.h.4.ln_1.bias: 768\n",
            "transformer.h.4.attn.c_attn.weight: 1769472\n",
            "transformer.h.4.attn.c_attn.bias: 2304\n",
            "transformer.h.4.attn.c_proj.weight: 589824\n",
            "transformer.h.4.attn.c_proj.bias: 768\n",
            "transformer.h.4.ln_2.weight: 768\n",
            "transformer.h.4.ln_2.bias: 768\n",
            "transformer.h.4.mlp.c_fc.weight: 2359296\n",
            "transformer.h.4.mlp.c_fc.bias: 3072\n",
            "transformer.h.4.mlp.c_proj.weight: 2359296\n",
            "transformer.h.4.mlp.c_proj.bias: 768\n",
            "transformer.h.5.ln_1.weight: 768\n",
            "transformer.h.5.ln_1.bias: 768\n",
            "transformer.h.5.attn.c_attn.weight: 1769472\n",
            "transformer.h.5.attn.c_attn.bias: 2304\n",
            "transformer.h.5.attn.c_proj.weight: 589824\n",
            "transformer.h.5.attn.c_proj.bias: 768\n",
            "transformer.h.5.ln_2.weight: 768\n",
            "transformer.h.5.ln_2.bias: 768\n",
            "transformer.h.5.mlp.c_fc.weight: 2359296\n",
            "transformer.h.5.mlp.c_fc.bias: 3072\n",
            "transformer.h.5.mlp.c_proj.weight: 2359296\n",
            "transformer.h.5.mlp.c_proj.bias: 768\n",
            "transformer.h.6.ln_1.weight: 768\n",
            "transformer.h.6.ln_1.bias: 768\n",
            "transformer.h.6.attn.c_attn.weight: 1769472\n",
            "transformer.h.6.attn.c_attn.bias: 2304\n",
            "transformer.h.6.attn.c_proj.weight: 589824\n",
            "transformer.h.6.attn.c_proj.bias: 768\n",
            "transformer.h.6.ln_2.weight: 768\n",
            "transformer.h.6.ln_2.bias: 768\n",
            "transformer.h.6.mlp.c_fc.weight: 2359296\n",
            "transformer.h.6.mlp.c_fc.bias: 3072\n",
            "transformer.h.6.mlp.c_proj.weight: 2359296\n",
            "transformer.h.6.mlp.c_proj.bias: 768\n",
            "transformer.h.7.ln_1.weight: 768\n",
            "transformer.h.7.ln_1.bias: 768\n",
            "transformer.h.7.attn.c_attn.weight: 1769472\n",
            "transformer.h.7.attn.c_attn.bias: 2304\n",
            "transformer.h.7.attn.c_proj.weight: 589824\n",
            "transformer.h.7.attn.c_proj.bias: 768\n",
            "transformer.h.7.ln_2.weight: 768\n",
            "transformer.h.7.ln_2.bias: 768\n",
            "transformer.h.7.mlp.c_fc.weight: 2359296\n",
            "transformer.h.7.mlp.c_fc.bias: 3072\n",
            "transformer.h.7.mlp.c_proj.weight: 2359296\n",
            "transformer.h.7.mlp.c_proj.bias: 768\n",
            "transformer.h.8.ln_1.weight: 768\n",
            "transformer.h.8.ln_1.bias: 768\n",
            "transformer.h.8.attn.c_attn.weight: 1769472\n",
            "transformer.h.8.attn.c_attn.bias: 2304\n",
            "transformer.h.8.attn.c_proj.weight: 589824\n",
            "transformer.h.8.attn.c_proj.bias: 768\n",
            "transformer.h.8.ln_2.weight: 768\n",
            "transformer.h.8.ln_2.bias: 768\n",
            "transformer.h.8.mlp.c_fc.weight: 2359296\n",
            "transformer.h.8.mlp.c_fc.bias: 3072\n",
            "transformer.h.8.mlp.c_proj.weight: 2359296\n",
            "transformer.h.8.mlp.c_proj.bias: 768\n",
            "transformer.h.9.ln_1.weight: 768\n",
            "transformer.h.9.ln_1.bias: 768\n",
            "transformer.h.9.attn.c_attn.weight: 1769472\n",
            "transformer.h.9.attn.c_attn.bias: 2304\n",
            "transformer.h.9.attn.c_proj.weight: 589824\n",
            "transformer.h.9.attn.c_proj.bias: 768\n",
            "transformer.h.9.ln_2.weight: 768\n",
            "transformer.h.9.ln_2.bias: 768\n",
            "transformer.h.9.mlp.c_fc.weight: 2359296\n",
            "transformer.h.9.mlp.c_fc.bias: 3072\n",
            "transformer.h.9.mlp.c_proj.weight: 2359296\n",
            "transformer.h.9.mlp.c_proj.bias: 768\n",
            "transformer.h.10.ln_1.weight: 768\n",
            "transformer.h.10.ln_1.bias: 768\n",
            "transformer.h.10.attn.c_attn.weight: 1769472\n",
            "transformer.h.10.attn.c_attn.bias: 2304\n",
            "transformer.h.10.attn.c_proj.weight: 589824\n",
            "transformer.h.10.attn.c_proj.bias: 768\n",
            "transformer.h.10.ln_2.weight: 768\n",
            "transformer.h.10.ln_2.bias: 768\n",
            "transformer.h.10.mlp.c_fc.weight: 2359296\n",
            "transformer.h.10.mlp.c_fc.bias: 3072\n",
            "transformer.h.10.mlp.c_proj.weight: 2359296\n",
            "transformer.h.10.mlp.c_proj.bias: 768\n",
            "transformer.h.11.ln_1.weight: 768\n",
            "transformer.h.11.ln_1.bias: 768\n",
            "transformer.h.11.attn.c_attn.weight: 1769472\n",
            "transformer.h.11.attn.c_attn.bias: 2304\n",
            "transformer.h.11.attn.c_proj.weight: 589824\n",
            "transformer.h.11.attn.c_proj.bias: 768\n",
            "transformer.h.11.ln_2.weight: 768\n",
            "transformer.h.11.ln_2.bias: 768\n",
            "transformer.h.11.mlp.c_fc.weight: 2359296\n",
            "transformer.h.11.mlp.c_fc.bias: 3072\n",
            "transformer.h.11.mlp.c_proj.weight: 2359296\n",
            "transformer.h.11.mlp.c_proj.bias: 768\n",
            "transformer.ln_f.weight: 768\n",
            "transformer.ln_f.bias: 768\n",
            "score.weight: 1536\n"
          ]
        }
      ],
      "source": [
        "# Print all the layers of this GPT2 model and the number of parameters per layer\n",
        "# If this is False, fine tune just the classifier layer and leave all other GPT2 parameters alone\n",
        "# If this is True, fine tune everything\n",
        "fine_tune = True\n",
        "\n",
        "# Turn off gradients using the above\n",
        "total_parameters = 0\n",
        "\n",
        "# Iterate through all model parameters\n",
        "for name, param in model.named_parameters():\n",
        "    # Print the parameter's layer and its total parameter count\n",
        "    param_count = param.numel()\n",
        "    print(f\"{name}: {param_count}\")\n",
        "    total_parameters += param_count\n",
        "\n",
        "    # Depending on the fine_tune flag, disable gradients for parameters not in the classifier\n",
        "    if not fine_tune and 'score' not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "assert(total_parameters == 124441344)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dd1c248c",
      "metadata": {
        "id": "dd1c248c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "765fc3a1",
      "metadata": {
        "id": "765fc3a1"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1fd8d459",
      "metadata": {
        "id": "1fd8d459"
      },
      "outputs": [],
      "source": [
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "bf0b712e",
      "metadata": {
        "id": "bf0b712e"
      },
      "outputs": [],
      "source": [
        "# Use torchmetrics to set up accuracy, recall, precision, and auroc\n",
        "accuracy = torchmetrics.Accuracy(num_classes=2, average='macro', task='binary').to(device)\n",
        "recall = torchmetrics.Recall(num_classes=2, average='macro', task='binary').to(device)\n",
        "precision = torchmetrics.Precision(num_classes=2, average='macro', task='binary').to(device)\n",
        "auroc = torchmetrics.AUROC(num_classes=2, average='macro', task='binary').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "084519f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "084519f5",
        "outputId": "1d30e2fb-83d8-4551-92c3-b5e30465e167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
            "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
            "Epoch:  50%|█████     | 1/2 [00:39<00:39, 39.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.1546\n",
            "\t - Validation Accuracy: 0.9812\n",
            "\t - Validation Precision: 0.7724\n",
            "\t - Validation Recall: 0.8060\n",
            "\t - Validation AUROC: 0.9036\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 2/2 [01:16<00:00, 38.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.0326\n",
            "\t - Validation Accuracy: 0.9875\n",
            "\t - Validation Precision: 0.8031\n",
            "\t - Validation Recall: 0.8571\n",
            "\t - Validation AUROC: 0.9065\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Main training / validation loop\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "\n",
        "    # ========== Training ==========\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Put each element of batch onto the device\n",
        "        batch = tuple(e.to(device) for e in batch)\n",
        "\n",
        "        # Unpack the batch\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Set gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss = train_output.loss\n",
        "        logits = train_output.logits\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_auroc = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(e.to(device) for e in batch)\n",
        "\n",
        "        # Unpack the batch\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "            eval_output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            logits = eval_output.logits\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        labels = b_labels\n",
        "        predicted_labels = torch.argmax(logits, axis=1)\n",
        "\n",
        "        val_accuracy.append(accuracy(predicted_labels, labels))\n",
        "        val_recall.append(precision(predicted_labels, labels))\n",
        "        val_precision.append(recall(predicted_labels, labels))\n",
        "        val_auroc.append(auroc(logits.softmax(dim=1)[:, 1], labels))\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)))\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)))\n",
        "    print('\\t - Validation AUROC: {:.4f}\\n'.format(sum(val_auroc)/len(val_auroc)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c029bd94",
      "metadata": {
        "id": "c029bd94"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "13d64bc6",
      "metadata": {
        "id": "13d64bc6"
      },
      "source": [
        "### Test on a specific sentence, see the outcome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "6dfcd9e7",
      "metadata": {
        "id": "6dfcd9e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1aa6c86-c4c0-4297-9cb6-94b8056c843d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence:  WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
            "Predicted Class:  Spam\n"
          ]
        }
      ],
      "source": [
        "new_sentence = 'WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.'\n",
        "\n",
        "# We need Token IDs and Attention Mask for inference on the new sentence\n",
        "test_ids = []\n",
        "test_attention_mask = []\n",
        "\n",
        "# Apply the tokenizer\n",
        "encoding = preprocessing(new_sentence, tokenizer)\n",
        "\n",
        "# Extract IDs and Attention Mask\n",
        "test_ids.append(encoding['input_ids'])\n",
        "test_attention_mask.append(encoding['attention_mask'])\n",
        "test_ids = torch.cat(test_ids, dim = 0)\n",
        "test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
        "\n",
        "# Forward pass, calculate logit predictions\n",
        "with torch.no_grad():\n",
        "    output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
        "\n",
        "prediction = 'Spam' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Ham'\n",
        "\n",
        "print('Input Sentence: ', new_sentence)\n",
        "print('Predicted Class: ', prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e4453061",
      "metadata": {
        "id": "e4453061"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a0c41741",
      "metadata": {
        "id": "a0c41741"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "875fab6d",
      "metadata": {
        "id": "875fab6d"
      },
      "source": [
        "Question 1: Run the above by fine tuning GPT2 and the classfier head and by not doing this (using GPT2 as a feature encoder). What is the gap between this? What are the metrics we get in each case?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae772301",
      "metadata": {
        "id": "ae772301"
      },
      "source": [
        "Solution: FILL Paste your output here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### No Fune Tune:"
      ],
      "metadata": {
        "id": "Bja4bUq1g9Jh"
      },
      "id": "Bja4bUq1g9Jh"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "071f0f80",
      "metadata": {
        "id": "071f0f80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee03ac8-40dd-4a3b-e61d-d673eebaa4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  50%|█████     | 1/2 [00:10<00:10, 10.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.4012\n",
            "\t - Validation Accuracy: 0.8599\n",
            "\t - Validation Precision: 0.0267\n",
            "\t - Validation Recall: 0.0643\n",
            "\t - Validation AUROC: 0.7705\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 2/2 [00:21<00:00, 10.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.3012\n",
            "\t - Validation Accuracy: 0.8670\n",
            "\t - Validation Precision: 0.1352\n",
            "\t - Validation Recall: 0.2119\n",
            "\t - Validation AUROC: 0.7957\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "config = GPT2Config.from_pretrained('gpt2',\n",
        "                                    output_attentions=False,\n",
        "                                    output_hidden_states=False,\n",
        "                                    num_labels=2)\n",
        "\n",
        "# Set to 'gpt2' (the smallest GPT2 which is 120 M parameters)\n",
        "# Use the config above and set other labels as needed\n",
        "model = GPT2ForSequenceClassification(config)\n",
        "\n",
        "# Set the pad token id to the eos token id\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5\n",
        "# See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr = 5e-5,\n",
        "    eps = 1e-08\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "# Print all the layers of this GPT2 model and the number of parameters per layer\n",
        "# If this is False, fine tune just the classifier layer and leave all other GPT2 parameters alone\n",
        "# If this is True, fine tune everything\n",
        "fine_tune = False\n",
        "\n",
        "# Turn off gradients using the above\n",
        "# total_parameters = 0\n",
        "\n",
        "# Iterate through all model parameters\n",
        "for name, param in model.named_parameters():\n",
        "    # Print the parameter's layer and its total parameter count\n",
        "    param_count = param.numel()\n",
        "    # print(f\"{name}: {param_count}\")\n",
        "    total_parameters += param_count\n",
        "\n",
        "    # Depending on the fine_tune flag, disable gradients for parameters not in the classifier\n",
        "    if not fine_tune and 'score' not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "# Main training / validation loop\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "\n",
        "    # ========== Training ==========\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Put each element of batch onto the device\n",
        "        batch = tuple(e.to(device) for e in batch)\n",
        "\n",
        "        # Unpack the batch\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Set gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss = train_output.loss\n",
        "        logits = train_output.logits\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_auroc = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(e.to(device) for e in batch)\n",
        "\n",
        "        # Unpack the batch\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "            eval_output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            logits = eval_output.logits\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        labels = b_labels\n",
        "        predicted_labels = torch.argmax(logits, axis=1)\n",
        "\n",
        "        val_accuracy.append(accuracy(predicted_labels, labels))\n",
        "        val_recall.append(precision(predicted_labels, labels))\n",
        "        val_precision.append(recall(predicted_labels, labels))\n",
        "        val_auroc.append(auroc(logits.softmax(dim=1)[:, 1], labels))\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)))\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)))\n",
        "    print('\\t - Validation AUROC: {:.4f}\\n'.format(sum(val_auroc)/len(val_auroc)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the comparison of not fine tune and fine tune all is as follows:"
      ],
      "metadata": {
        "id": "6Y4HaRjWnW2B"
      },
      "id": "6Y4HaRjWnW2B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Not Fine Tune:**"
      ],
      "metadata": {
        "id": "GkRWFtQ1niOV"
      },
      "id": "GkRWFtQ1niOV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch:  50%|█████     | 1/2 [00:10<00:10, 10.64s/it]\n",
        "\t - Train loss: 0.4012\n",
        "\t - Validation Accuracy: 0.8599\n",
        "\t - Validation Precision: 0.0267\n",
        "\t - Validation Recall: 0.0643\n",
        "\t - Validation AUROC: 0.7705\n",
        "\n",
        "Epoch: 100%|██████████| 2/2 [00:21<00:00, 10.89s/it]\n",
        "\t - Train loss: 0.3012\n",
        "\t - Validation Accuracy: 0.8670\n",
        "\t - Validation Precision: 0.1352\n",
        "\t - Validation Recall: 0.2119\n",
        "\t - Validation AUROC: 0.7957\n"
      ],
      "metadata": {
        "id": "sY11bXaRnmaz"
      },
      "id": "sY11bXaRnmaz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine Tune:**"
      ],
      "metadata": {
        "id": "fDQ-OVZGnxUM"
      },
      "id": "fDQ-OVZGnxUM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch:  50%|█████     | 1/2 [00:39<00:39, 39.63s/it]\n",
        "\t - Train loss: 0.1546\n",
        "\t - Validation Accuracy: 0.9812\n",
        "\t - Validation Precision: 0.7724\n",
        "\t - Validation Recall: 0.8060\n",
        "\t - Validation AUROC: 0.9036\n",
        "\n",
        "Epoch: 100%|██████████| 2/2 [01:16<00:00, 38.45s/it]\n",
        "\t - Train loss: 0.0326\n",
        "\t - Validation Accuracy: 0.9875\n",
        "\t - Validation Precision: 0.8031\n",
        "\t - Validation Recall: 0.8571\n",
        "\t - Validation AUROC: 0.9065"
      ],
      "metadata": {
        "id": "sBSR2ZWIn3CJ"
      },
      "id": "sBSR2ZWIn3CJ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MCUboPsjhFbI"
      },
      "id": "MCUboPsjhFbI",
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}